version: '3.4'

services:
  proxy:
    image: nginx:1.21.0-alpine
    container_name: proxy
    volumes:
      - .config/proxy/default.conf:/etc/nginx/templates/default.conf.template
    ports:
      - $PROXY_PORT:80
    depends_on:
      - auth-keycloak-acl
      - employee-service
      - time-record-service
    networks:
      - timeclock

  auth-keycloak-acl:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      ENV: dev
      ELASTIC_APM_SERVICE_NAME: auth-keycloak-acl
    ports:
      - $AUTH_GRPC_PORT:50051
      - $AUTH_REST_PORT:8080
    volumes:
      # - service/auth-keycloak-acl:/go/src/
      - ../auth-keycloak-acl:/go/src/
    depends_on:
      - keycloak
    networks:
      - timeclock
    extra_hosts:
      - 'host.docker.internal:172.17.0.1'

  keycloak:
    image: quay.io/keycloak/keycloak:latest
    environment:
      DB_VENDOR: POSTGRES
      DB_ADDR: keycloakdb
      DB_DATABASE: $KEYCLOAK_DB
      DB_USER: $KEYCLOAK_USERNAME
      DB_SCHEMA: public
      DB_PASSWORD: $KEYCLOAK_PASSWORD
      KEYCLOAK_USER: $KEYCLOAK_USERNAME
      KEYCLOAK_PASSWORD: $KEYCLOAK_PASSWORD
      # Uncomment the line below if you want to specify JDBC parameters. The parameter below is just an example, and it shouldn't be used in production without knowledge. It is highly recommended that you read the PostgreSQL JDBC driver documentation in order to use it.
      # JDBC_PARAMS: "ssl=true"
    ports:
      - 8080:8080
    depends_on:
      - keycloakdb
    networks:
      - timeclock

  keycloakdb:
    image: postgres
    volumes:
      - pgauthdb:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: $KEYCLOAK_DB
      POSTGRES_USER: $KEYCLOAK_USERNAME
      POSTGRES_PASSWORD: $KEYCLOAK_PASSWORD
    networks:
      - timeclock

  time-record-service:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      ENV: dev
      ELASTIC_APM_SERVICE_NAME: time-record-service
    ports:
      - $TIME_RECORD_GRPC_PORT:50051
      - $TIME_RECORD_REST_PORT:8080
    volumes:
      # - service/time-record-service:/go/src/
      - ../time-record-service:/go/src/
    depends_on:
      - trdb
    networks:
      - timeclock
    extra_hosts:
      - 'host.docker.internal:172.17.0.1'

  trdb:
    image: mongo:4.4
    restart: always
    command: mongod --auth
    tty: true
    environment:
      MONGO_INITDB_ROOT_USERNAME: $MONGODB_USERNAME
      MONGO_INITDB_ROOT_PASSWORD: $MONGODB_PASSWORD
      MONGO_INITDB_DATABASE: $DB_NAME
      MONGODB_DATA_DIR: /data/db
      MONDODB_LOG_DIR: /dev/null
    volumes:
      - mgdata:/data/db
    ports:
      - $DB_PORT:27017
    networks:
      - timeclock

  mongo-express:
    image: mongo-express
    tty: true
    environment:
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin123
      ME_CONFIG_MONGODB_PORT: 27017
      ME_CONFIG_MONGODB_SERVER: trdb
      ME_CONFIG_MONGODB_ADMINUSERNAME: $MONGODB_USERNAME
      ME_CONFIG_MONGODB_ADMINPASSWORD: $MONGODB_PASSWORD
    ports:
      - 8081:8081
    depends_on:
      - trdb
    networks:
      - timeclock

  employee-service:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      ENV: dev
      ELASTIC_APM_SERVICE_NAME: employee-service
    ports:
      - $EMPLOYEE_GRPC_PORT:50051
      - $EMPLOYEE_REST_PORT:8080
    volumes:
      # - service/employee-service:/go/src/
      - ../employee-service:/go/src/
    depends_on:
      - keycloak
    networks:
      - timeclock
    extra_hosts:
      - 'host.docker.internal:172.17.0.1'

  # APM
  apm-server:
    image: docker.elastic.co/apm/apm-server:7.12.1
    depends_on:
      - elasticsearch
      - kibana
    cap_add: ['CHOWN', 'DAC_OVERRIDE', 'SETGID', 'SETUID']
    cap_drop: ['ALL']
    ports:
      - 8200:8200
    networks:
      - timeclock
    environment:
      - .config/elastic/apm-server.yml:/usr/share/apm-server/apm-server.yml
    healthcheck:
      interval: 10s
      retries: 12
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:8200/

  # filebeat:
  #   image: "docker.elastic.co/beats/filebeat:5.5.1"
  #   volumes:
  #    - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml
  #    - filebeatdata:/usr/share/filebeat/data ## mounting paths for persistent data (log progress etc)
  #    - ./logs:/mnt/log  ## this is the logs volume path, just to stick with the original
  #   depends_on:
  #     - logstash

  #   logstash:
  #     image: "docker.elastic.co/logstash/logstash:7.2.0"
  #     volumes:
  #       - ./config/logstash.yml:/usr/share/logstash/config/logstash.yml
  #       - ./config/logstash-pipelines.yml:/usr/share/logstash/config/pipelines.yml
  #       - ./config/logstash-pipeline:/usr/share/logstash/pipeline/  ## this is the pipeline config location
  #       - ./logs:/mnt/logs  ## this is the logs volume path, just to stick with the original
  #       - logstashdata:/usr/share/logstash/data
  #     environment:
  #       - "LS_HEAP_SIZE=-Xms512m -Xmx512m"
  #     # command: bin/logstash -f pipeline/logstash.conf
  #     ports:
  #       - 9600:9600  ## this is the metrics endpoint port
  #     depends_on:
  #       - elasticsearch

  kibana:
    image: 'docker.elastic.co/kibana/kibana:7.12.1'
    ports:
      - 5601:5601
    volumes:
      - .config/elastic/kibana.yml:/usr/share/kibana/config/kibana.yml
    depends_on:
      - elasticsearch
    networks:
      - timeclock
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --write-out 'HTTP %{http_code}' --fail --silent --output /dev/null http://localhost:5601/api/status

  elasticsearch:
    image: 'docker.elastic.co/elasticsearch/elasticsearch:7.12.1'
    ports:
      - 9200:9200
    volumes:
      - .config/elastic/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      #   - ./.elastic/elasticsearch-ik-plugin:/usr/share/elasticsearch/plugins/ik
      - esdata:/usr/share/elasticsearch/data
    environment:
      - 'ES_JAVA_OPTS=-Xms512m -Xmx512m' ## setting jvm heap memory limits, based on the type of nodes (master, data, etc.), you can tweak this to optimize resource usage vs. performance
      - node.name=es-master ## node name needs to be unique, so setting it in env vars
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - timeclock
    healthcheck:
      interval: 20s
      retries: 10
      test: curl -s http://localhost:9200/_cluster/health | grep -vq '"status":"red"'

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - timeclock

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
      - 9094:9094
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENERS: INTERNAL://:9092,OUTSIDE://:9094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,OUTSIDE://host.docker.internal:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT
    networks:
      - timeclock
    extra_hosts:
      - 'host.docker.internal:172.17.0.1'

  kafka-topics-generator:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - kafka
    command: >
      bash -c
        "sleep 5s &&
        kafka-topics --create --topic=employees --if-not-exists --bootstrap-server=kafka:9092"
    networks:
      - timeclock

  control-center:
    image: confluentinc/cp-enterprise-control-center:6.0.1
    hostname: control-center
    depends_on:
      - kafka
    ports:
      - 9021:9021
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka:9092
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      PORT: 9021
    networks:
      - timeclock

volumes:
  esdata:
    driver: local
  pgauthdb:
    driver: local
  mgdata:
    driver: local

networks:
  timeclock:
    driver: bridge
